{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Load the first and second JSON files\n",
    "with open(r'C:\\Users\\leonc\\Thesis 2024\\Toxic-Symbology\\Text Clustering\\ontoxGPAHE.json', 'r') as f1, open(r'C:\\Users\\leonc\\Thesis 2024\\Toxic-Symbology\\tools\\OnToxMeme_dict.json', 'r') as f2:\n",
    "    data1 = json.load(f1)\n",
    "    data2 = json.load(f2)\n",
    "\n",
    "# Extract keys from the second file that have \"Referenced_in_meme\"\n",
    "keys_with_references = {\n",
    "    key for key, value in data2.items() if \"Referenced_in_meme\" in value\n",
    "}\n",
    "\n",
    "# Filter entries from the first file based on these keys\n",
    "filtered_entries = {\n",
    "    key: value for key, value in data1.items() if key in keys_with_references\n",
    "}\n",
    "\n",
    "# Save the filtered entries to a new JSON file\n",
    "with open('ontoxGPAHE_56.json', 'w') as output:\n",
    "    json.dump(filtered_entries, output, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:\\Users\\leonc\\Thesis 2024\\Toxic-Symbology\\Text Clustering\\ontoxGPAHE_56.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "descriptions_1 = [entry[\"Description\"] for entry in data.values()]\n",
    "descriptions_2 = [entry[\"Description\"] + entry[\"Title\"] for entry in data.values()]\n",
    "descriptions_3 = [entry[\"Description\"] + entry[\"Title\"] + entry[\"Ideology\"] for entry in data.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a05fd61b12f84719a566c117d3588a91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68e6bc42d77843dabe6df5f02fc6d98e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc7eed9ad41e44d98092b4453855477c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sentence_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "embeddings_1 = sentence_model.encode(descriptions_1, show_progress_bar=True)\n",
    "\n",
    "sentence_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "embeddings_2 = sentence_model.encode(descriptions_2, show_progress_bar=True)\n",
    "\n",
    "sentence_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "embeddings_3 = sentence_model.encode(descriptions_3, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality Reduction\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=10)\n",
    "\n",
    "# Clustering (this is where the number of topics are generated, if we boost min_cluster_size we get fewer topics; we can also use a different cluster algorithm)\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=5, min_samples=2, metric='euclidean', cluster_selection_method='eom')     # the default algorithm\n",
    "# hdbscan_model = KMeans(n_clusters=50)                                                                               # K-Means allows us to force every document into a cluster\n",
    "\n",
    "# Tokenizing (stopwords do help, and we can also add n-grams, and add a document frequency cutoff)\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\", ngram_range=(1, 2))\n",
    "\n",
    "# Weighting Scheme (we'll use the same as in the basic version)\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "\n",
    "# Representation Tuning (makes the extracted keywords better)\n",
    "representation_model = KeyBERTInspired()\n",
    "\n",
    "# Train our topic model with BERTopic\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=sentence_model,\n",
    "    ctfidf_model=ctfidf_model,\n",
    "    representation_model=representation_model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    vectorizer_model=vectorizer_model\n",
    ")\n",
    "\n",
    "topics_1, probs_1 = topic_model.fit_transform(descriptions_1, embeddings_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 20:43:48,963 - BERTopic - WARNING: Using a custom list of topic assignments may lead to errors if topic reduction techniques are used afterwards. Make sure that manually assigning topics is the last step in the pipeline.Note that topic embeddings will also be created through weightedc-TF-IDF embeddings instead of centroid embeddings.\n"
     ]
    }
   ],
   "source": [
    "new_topics_1 = topic_model.reduce_outliers(descriptions_1, topics_1, strategy=\"c-tf-idf\")\n",
    "topic_model.update_topics(descriptions_1, topics=new_topics_1, vectorizer_model=vectorizer_model)\n",
    "file = topic_model.get_topic_info()\n",
    "file.to_csv(\"description.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('white', 0.04388445138418539),\n",
       " ('term', 0.043465373854853086),\n",
       " ('racist', 0.038755921696056315),\n",
       " ('people', 0.03760234005512059),\n",
       " ('lgbtq', 0.03207332730907055),\n",
       " ('used', 0.030746749222875226),\n",
       " ('anti', 0.025889581685194365),\n",
       " ('4chan', 0.025255584173958043),\n",
       " ('trans', 0.023020331993949653),\n",
       " ('women', 0.022236623215192857)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topic(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train our topic model with BERTopic\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=sentence_model,\n",
    "    ctfidf_model=ctfidf_model,\n",
    "    representation_model=representation_model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    vectorizer_model=vectorizer_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_2, probs_2 = topic_model.fit_transform(descriptions_2, embeddings_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 20:44:47,933 - BERTopic - WARNING: Using a custom list of topic assignments may lead to errors if topic reduction techniques are used afterwards. Make sure that manually assigning topics is the last step in the pipeline.Note that topic embeddings will also be created through weightedc-TF-IDF embeddings instead of centroid embeddings.\n"
     ]
    }
   ],
   "source": [
    "new_topics_2 = topic_model.reduce_outliers(descriptions_2, topics_2, strategy=\"c-tf-idf\")\n",
    "topic_model.update_topics(descriptions_2, topics=new_topics_2, vectorizer_model=vectorizer_model)\n",
    "file = topic_model.get_topic_info()\n",
    "file.to_csv(\"description_title.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lgb', 0.05118351331695788),\n",
       " ('partner', 0.05118351331695788),\n",
       " ('scientific', 0.05118351331695788),\n",
       " ('trans', 0.0467576395388898),\n",
       " ('people', 0.046430170280825966),\n",
       " ('acronym', 0.03646749440404747),\n",
       " ('ywnbaw woman', 0.03646749440404747),\n",
       " ('explains', 0.03646749440404747),\n",
       " ('finding', 0.03646749440404747),\n",
       " ('globohomo', 0.03646749440404747)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topic(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train our topic model with BERTopic\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=sentence_model,\n",
    "    ctfidf_model=ctfidf_model,\n",
    "    representation_model=representation_model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    vectorizer_model=vectorizer_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_3, probs_3 = topic_model.fit_transform(descriptions_3, embeddings_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 20:49:59,911 - BERTopic - WARNING: Using a custom list of topic assignments may lead to errors if topic reduction techniques are used afterwards. Make sure that manually assigning topics is the last step in the pipeline.Note that topic embeddings will also be created through weightedc-TF-IDF embeddings instead of centroid embeddings.\n"
     ]
    }
   ],
   "source": [
    "# new_topics_3 = topic_model.reduce_outliers(descriptions_3, topics_3, strategy=\"c-tf-idf\")\n",
    "topic_model.update_topics(descriptions_3, topics=new_topics_3, vectorizer_model=vectorizer_model)\n",
    "file = topic_model.get_topic_info()\n",
    "file.to_csv(\"description_title_ideology.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('theory', 0.05810835371490405),\n",
       " ('conspiracy', 0.054099676752202404),\n",
       " ('world', 0.049181524320184),\n",
       " ('great', 0.043238715966755874),\n",
       " ('21', 0.03977728205066006),\n",
       " ('agenda', 0.03783387647091139),\n",
       " ('global', 0.03783387647091139),\n",
       " ('agenda 21', 0.035117861565955207),\n",
       " ('government', 0.034094813186280055),\n",
       " ('conspiracy theory', 0.032429036975066904)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_model.get_topic(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toxic symbols in each topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the symbol data from the JSON file\n",
    "with open(r'C:\\Users\\leonc\\Thesis 2024\\Toxic-Symbology\\Text Clustering\\ontoxGPAHE_56.json', 'r') as f:\n",
    "    symbol_data = json.load(f)\n",
    "\n",
    "# Create a list of symbols and their descriptions from the JSON file\n",
    "symbols = []\n",
    "for symbol_id, symbol_info in symbol_data.items():\n",
    "    symbols.append({\n",
    "        \"Symbol_ID\": symbol_id,\n",
    "        \"Title\": symbol_info.get(\"Title\", \"\"),\n",
    "        \"Description\": symbol_info.get(\"Description\", \"\"),\n",
    "    })\n",
    "\n",
    "# Create a DataFrame for symbols\n",
    "symbol_df_1 = pd.DataFrame(symbols)\n",
    "\n",
    "# Ensure that 'topics_1' (the topic assignments from BERTopic) matches the order of the symbols\n",
    "# The length of 'topics_1' must be the same as the number of symbols (descriptions_1)\n",
    "symbol_df_1['Topic'] = new_topics_1  # topics_1 should be the topic assignments from BERTopic\n",
    "\n",
    "# Now, group by the topic to see which symbols belong to each topic\n",
    "symbols_per_topic_1 = symbol_df_1.groupby('Topic').agg({\n",
    "    'Symbol_ID': 'unique', \n",
    "    'Title': 'unique', \n",
    "    'Description': 'unique'\n",
    "})\n",
    "\n",
    "# Create a DataFrame for symbols\n",
    "symbol_df_2 = pd.DataFrame(symbols)\n",
    "\n",
    "# Ensure that 'topics_1' (the topic assignments from BERTopic) matches the order of the symbols\n",
    "# The length of 'topics_1' must be the same as the number of symbols (descriptions_1)\n",
    "symbol_df_2['Topic'] = new_topics_2  # topics_1 should be the topic assignments from BERTopic\n",
    "\n",
    "# Now, group by the topic to see which symbols belong to each topic\n",
    "symbols_per_topic_2 = symbol_df_2.groupby('Topic').agg({\n",
    "    'Symbol_ID': 'unique', \n",
    "    'Title': 'unique', \n",
    "    'Description': 'unique'\n",
    "})\n",
    "\n",
    "# Create a DataFrame for symbols\n",
    "symbol_df_3 = pd.DataFrame(symbols)\n",
    "\n",
    "# Ensure that 'topics_1' (the topic assignments from BERTopic) matches the order of the symbols\n",
    "# The length of 'topics_1' must be the same as the number of symbols (descriptions_1)\n",
    "symbol_df_3['Topic'] = new_topics_3  # topics_1 should be the topic assignments from BERTopic\n",
    "\n",
    "# Now, group by the topic to see which symbols belong to each topic\n",
    "symbols_per_topic_3 = symbol_df_3.groupby('Topic').agg({\n",
    "    'Symbol_ID': 'unique', \n",
    "    'Title': 'unique', \n",
    "    'Description': 'unique'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Agenda 21', 'Cultural Marxism',\n",
       "       'Grand Remplacement / être grand remplacé (Great Replacement)',\n",
       "       'I will not eat the bugs', 'Revolt Against The Modern World',\n",
       "       'You will own nothing and be happy', 'ZOG'], dtype=object)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbols_per_topic_3[\"Title\"][4]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
